# 96.8±2.1(0.5,-1,-1)
telegram/:
#  net: ScaleNet         # QiGi3
  nonlinear: 1
  layer: 4
  lr: 0.01
  normalize: 1
#  BN_model: 0
  BN_model: 1    # sure
  First_self_loop: 'add'
  jk: 'max'
  dropout: 0.0
  jk_inner: 0
#  alphaDir: 0.5     # 0.5, 0.2(94.8)
#  betaDir: -1
#  gamaDir: -1



# (1,-1,-1@79.6±1.7)--(1,1,1@79.1±1.5)
WikipediaNetwork/chameleon:     # 1,2,1----1,2,-1------1,1,2-----1,-1,1---(1,-1,-1@79.6±1.7)--(1,1,1@79.1±1.5)
  dropout: 0.0  # 0.5(acc65), 0.0(acc75)
  nonlinear: 1
#  net: ScaleNet
  layer: 5      # 2(78.9), 5(78.8)
  lr: 0.005
  normalize: 1
#  BN_model: 0
  BN_model: 1 # without layerNorm, very bad(but Dir-GNN prefer no LN)
  First_self_loop: "remove"
  jk: 'max'
  jk_inner: 0
#  alphaDir: 1
#  betaDir: 1
#  gamaDir: 1

# (1_1_-1) 74.7±2.4,   (1,-1,-1) 75.2±1.7
WikipediaNetwork/squirrel:
  nonlinear: 1
#  normalize: True    # for DirGNN
  dropout: 0.0  # 0.5(acc72.7), 0.0(acc75)
#  net: ScaleNet
  layer: 4
  lr: 0.01
  normalize: 1
#  BN_model: 0
  BN_model: 1 # 0(48.9), 1(72)
  First_self_loop: "remove"     # Sure
  jk: 'max'
  jk_inner: 0
#  alphaDir: 1   # best is 1,1,-1
#  betaDir: -1    #-1
#  gamaDir: -1    # -1




#
cora_ml/:     # 2,0.5,-1---2.0_0.5_2.0--2.0_0.5_0.5--2.0_-1.0_-1.0
  nonlinear: 1
  net: ScaleNet   #  net: QiGi3
  feat_dim: 128   # better than 64
  layer: 3   # 2(80), 3(75), 4(79.8), 5(79.6)
  lr: 0.01    # 0.1(79), 0.01(80.7)
  normalize: 0
  BN_model: 0   # 0(29), 1(79)  # 1(79), 0(82.7)
  First_self_loop : 'add'  # sure
  jk: "max"     # max(80.7), cat(79.8), 0(76.9)
  jk_inner: 0
  dropout: 0.5   # 0.0(80.51), 0.5(80.76)
  alphaDir: 2   # 0.5,1.-1 before having 2
  betaDir: -1  # 1
  gamaDir: -1  # -1 , 0.5

#
citeseer_npz/:        # 0.5,2,0----1.0_0.0_0.5, ----1.0_2.0_0.5----2.0_2.0_0.0----2.0_0.0_0.5----2.0_0.0_0.0----2.0_2.0_0.5-----0.5_2.0_0.5---0.0_0.5_2.0--0.5_2.0_2.0
  net: ScaleNet
  jk_inner: 0
  nonlinear: 0    # 0(68.3), 1(67.6)
  feat_dim: 128
  layer: 3     # 1(65.3),2(64.8), 3(62.6), 4(64.0), 5(64.4)
  lr: 0.01
  normalize: 1    # 1(68.3), 0 much worse60.7,
  BN_model: 0   # 0(68.3), 1(66.9)  # for high performance 0 is better.
  First_self_loop : 'add'     # 'add'(64.75), 'remove'(58.2), 0(57.5)
  jk: 'max'   # 'max'(68.3). 'cat'(), 0(67.8)
  dropout: 0.5   # 0(61.44), 0.5(64.75)
  inci_norm: 'dir'    # dir(64.8), sym(63.9), row(63.8)
  alphaDir: 0.5   # 0,0,0.5 before having 2
  betaDir: 2
  gamaDir: -1


# 76.7±0.9
WikiCS/:
#  net: ScaleNet
  nonlinear: 1
  layer: 2
  lr: 0.01
  normalize: 1
  BN_model: 0  # 0(23), 1(73.5)
  First_self_loop : 0  # remove(73.9), add(73.7), 0(77.3±0.8)
  jk: 'max'   # max(75.9),cat(75.99), 0(73.5)
  jk_inner: 0
  dropout: 0.5  # 0.5(75.9), 0.0(73.9)
  alphaDir: 0.5
  betaDir: 2
  gamaDir: -1   # 0(76.5)

#
dgl/pubmed:
#  net: ScaleNet
  jk_inner: 'cat'   #cat(76.1±1.6), 0(75.9)
  nonlinear: 1
  layer: 4
  lr: 0.01
  normalize: 0
  BN_model: 0  # 0(33), 1(73)
  First_self_loop : 'add'
  jk: 'max'
  dropout: 0.0  #0.5(39), 0.0(77.5)
  alphaDir: 0.5   # 0.5(72.2), 0(73.2)
  betaDir: 0.5    #
  gamaDir: 0


## 91.23 ± 0.32
#directed-roman-empire:
#  lr: 0.01
#  feat_dim: 256
#  layer: 5
#  jk: "cat"
#  dropout: 0.2
#  patience: 200
#  conv_type: "dir-sage"